{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenchan88/CS466Lab2InfoRetrieval/blob/main/Lab2_InformationRetrieval_ipynb_jchanOldVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab Assignment: Document Retrieval System**\n",
        "**Due Date: April 26**\n",
        "\n",
        "**Objective:**\n",
        "The objective of this lab assignment is to implement a document retrieval system using the TF-IDF algorithm and cosine similarity distance. Students will learn to preprocess text data, calculate TF-IDF scores, compute cosine similarity, and retrieve the top relevant documents for a given query.\n",
        "This lab assignment is designed to enhance students' understanding of text processing, information retrieval, and machine learning techniques. It encourages hands-on experience with real-world text data and provides opportunities for experimentation and improvement. You can use the following helper code as a guideline or create your own implementation.\n",
        "\n",
        "Tasks:\n",
        "This lab is a continuation of Lab1. You can use the document_class pickel object. Implement classes to represent documents and document collections.\n",
        "Implement methods to preprocess documents and compute TF-IDF scores.\n",
        "Fetch documents from a URL and preprocess them.\n",
        "Implement methods to preprocess user queries and compute cosine similarity between the query and documents.\n",
        "Perform document retrieval for first 5 queries and display the top 20 relevant documents.\n",
        "\n",
        "**Expected Output**\n",
        "As an example, your implementation should give a similar output:\n",
        "\n",
        "Documents for query 3: [184, 359, 486, 686, 327, 875, 13, 102, 685, 540, 349, 1361, 25, 573, 758, 12, 252, 880, 57, 755]\n",
        "\n",
        "The numbers inside the parenthesis correspond to the top 20 documents.\n",
        "\n",
        "**Detailed Tasks:**\n",
        "\n",
        "1. **Text Preprocessing:**\n",
        "    - Implement a text preprocessing function that tokenizes text, removes punctuation, converts text to lowercase, and removes stop words.\n",
        "    - Ensure that the preprocessing function is capable of handling a collection of documents.\n",
        "    \n",
        "\n",
        "2. **TF-IDF Calculation:**\n",
        "- we need to load the document class pickle file to access the TF-IDF scores for each document.\n",
        "    - The TF-IDF scores are stored in this file, and by loading it, we can access the scores for each document and compare them with the query terms. This allows us to find which documents contain words similar to the ones in the query and retrieve their indexes.\n",
        "    - Implement a TF-IDF calculation function that computes the TF-IDF scores for each term in the document collection.\n",
        "    - Ensure that the TF-IDF calculation function normalizes the term frequencies and computes the inverse document frequency.\n",
        "    - Implement a class or function to represent the document vectors.\n",
        "\n",
        "3. **Document Retrieval:**\n",
        "    - Implement a cosine similarity function to compute the similarity between a query and each document in the collection.\n",
        "    - Use the TF-IDF vectors calculated in step 2 to compute the cosine similarity.\n",
        "    -- compute_cosine_similarity could be a potential name of this function. You need to complete this function\n",
        "    - Retrieve the top 20 relevant documents for each query using the files: queries.txt.\n",
        "\n",
        "4. **Data Loading and Saving:**\n",
        "    - Load the document collection from a given file (e.g., pickle file).\n",
        "    - Save the TF-IDF vectors to a pickle file for future use.\n",
        "\n",
        "5. **Integration and Testing:**\n",
        "    - Integrate all the implemented components to create a complete document retrieval system.\n",
        "    - Test the system using a sample query file (e.g., queries.txt).\n",
        "    - Ensure that the system produces correct results and handles edge cases effectively.\n",
        "\n",
        "**Submission:**\n",
        "- Submit the Python notebook file (.ipynb) containing the implementation of each component including from the first Lab where you created the document class, documents_collection pickel file.\n",
        "- Provide a README file explaining the usage of the code and any additional instructions.\n",
        "- Ensure that the code is well-commented and organized.\n"
      ],
      "metadata": {
        "id": "zCYk7G-X0wu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will create a class TextVector that contains the following methods:\n"
      ],
      "metadata": {
        "id": "_spH2WrgzX0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK (if not already installed)\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3uDLApZDjl-",
        "outputId": "7ba8d6a5-3e9d-4b4e-9e6a-358247e24f76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the NLTK module\n",
        "import nltk\n",
        "\n",
        "# Download the punkt resource (if not already downloaded)\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TYQsZb8Dkve",
        "outputId": "fee1a567-ca5c-4725-e88e-981e741d3e4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ4V8xbeDph1",
        "outputId": "d04a3963-fc72-4517-e550-f1fc02571f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import pickle\n",
        "import requests\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, doc_id, text):\n",
        "        self.doc_id = doc_id\n",
        "        self.text = text\n",
        "\n",
        "    def preprocess_text(self):\n",
        "        # Tokenize text\n",
        "        tokens = []\n",
        "        temp = word_tokenize(self.text)\n",
        "        cleanedTokens = []\n",
        "        # Remove punctuation and convert to lowercase\n",
        "        for word in temp:\n",
        "          if word.isalnum():\n",
        "            cleanedTokens.append(word.lower())\n",
        "        # Remove stop words\n",
        "        for w in cleanedTokens:\n",
        "          if w not in stopwords.words('english'):\n",
        "            tokens.append(w)\n",
        "        return tokens\n",
        "\n",
        "class DocumentCollection:\n",
        "    def __init__(self):\n",
        "        self.documents = {}\n",
        "\n",
        "    def add_document(self, doc):\n",
        "        self.documents[doc.doc_id] = doc\n",
        "\n",
        "    def preprocess_documents(self):\n",
        "        for doc_id, doc in self.documents.items():\n",
        "            self.documents[doc_id].text = doc.preprocess_text()\n",
        "\n",
        "    def compute_tf_idf(self):\n",
        "        tf_idf = {}\n",
        "        N = len(self.documents)\n",
        "        # Compute term frequency (TF) for each document\n",
        "        term_freqs = {}\n",
        "        for docID, doc in self.documents.items():\n",
        "          term_freqs[docID] = Counter(doc.text)\n",
        "        # Normalize term frequencies\n",
        "          totalTerms = sum(term_freqs[docID].values())\n",
        "          for term in term_freqs[docID]:\n",
        "            term_freqs[docID][term] = term_freqs[docID][term] / totalTerms\n",
        "\n",
        "        # Compute inverse document frequency (IDF) for each term\n",
        "        docFreqs = Counter()\n",
        "        ##give doc frqs same keys (terms) as in freq dictionary\n",
        "        for freq in term_freqs.values():\n",
        "          docFreqs.update(freq.keys())\n",
        "        IDF = {}\n",
        "        for term, df in docFreqs.items():\n",
        "          ## check if frequency is non-zero to avoid 0 divison err\n",
        "          if df != 0:\n",
        "            IDF[term] = math.log(N / df)\n",
        "          ## if dividing by 0, just return 0\n",
        "          else:\n",
        "            IDF[term] = 0\n",
        "        # Compute TF-IDF scores\n",
        "        for docID, freq in term_freqs.items():\n",
        "          tf_idf[docID] = {}\n",
        "          for term, tf in freq.items():\n",
        "            tf_idf[docID][term] = tf * IDF[term]\n",
        "\n",
        "        return tf_idf\n",
        "\n",
        "    def save_to_pickle(self, filename):\n",
        "        with open(filename, 'wb') as file:\n",
        "            pickle.dump(self.documents, file)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_from_pickle(filename):\n",
        "        with open(filename, 'rb') as file:\n",
        "            return pickle.load(file)\n",
        "\n",
        "def fetch_documents(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        text = response.text\n",
        "        return extract_documents(text)\n",
        "    else:\n",
        "        print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "        return None\n",
        "\n",
        "def extract_documents(text):\n",
        "    collection = DocumentCollection()\n",
        "    doc_id = None\n",
        "    text_buffer = \"\"\n",
        "    for line in text.splitlines():\n",
        "        if line.startswith(\".I\"):\n",
        "            if doc_id is not None:\n",
        "                collection.add_document(Document(doc_id, text_buffer))\n",
        "                text_buffer = \"\"\n",
        "            doc_id = int(line.strip().split()[-1])\n",
        "        elif line.startswith(\".W\"):\n",
        "            text_buffer += line.strip().replace(\".W\", \"\")\n",
        "        else:\n",
        "            text_buffer += line.strip()\n",
        "    if doc_id is not None:  # Add the last document\n",
        "        collection.add_document(Document(doc_id, text_buffer))\n",
        "    return collection\n",
        "\n",
        "def preprocess_query(query):\n",
        "    # Implement query preprocessing here (similar to document preprocessing)\n",
        "    # Tokenize, remove stop words, etc.\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(query)\n",
        "    cleanedTokens = []\n",
        "    query_terms = []\n",
        "\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    for word in tokens:\n",
        "        if word.isalnum():\n",
        "            cleanedTokens.append(word.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "\n",
        "    for w in cleanedTokens:\n",
        "          if w not in stopwords.words('english'):\n",
        "            query_terms.append(w)\n",
        "\n",
        "    return query_terms\n",
        "\n",
        "\n",
        "\n",
        "def compute_cosine_similarity(query, tf_idf):\n",
        "\n",
        "    # Normalize query vector\n",
        "    query_vector = {}\n",
        "    for term in query:\n",
        "        query_vector[term] = 1 / len(query)\n",
        "\n",
        "    # Compute cosine similarity between query and each document\n",
        "    cosSimilarityDict = {}\n",
        "    for docID, docVector in tf_idf.items():\n",
        "      # dotProd = np.dot(list(query_vector.values()), list(docVector.values()))\n",
        "      common_terms = set(query_vector.keys()) & set(docVector.keys())\n",
        "      ## Only calculate dot product for common terms\n",
        "      dotProduct = 0\n",
        "      for term in common_terms:\n",
        "          dotProduct += query_vector[term] * docVector[term]\n",
        "      queryMagnitude = sum(value ** 2 for value in query_vector.values()) ** 0.5\n",
        "      docMagnitude = sum(value ** 2 for value in docVector.values()) ** 0.5\n",
        "      # if query_vector.values() != 0 and docVector.values() !=0:\n",
        "      if queryMagnitude != 0 and docMagnitude != 0:\n",
        "        cosSim = dotProduct / (queryMagnitude * docMagnitude)\n",
        "      else:\n",
        "        cosSim = 0\n",
        "      cosSimilarityDict[docID] = cosSim\n",
        "    # Sort documents by similarity score\n",
        "    sorted_documents = []\n",
        "    # Sort documents by similarity score\n",
        "    sorted_documents = sorted(cosSimilarityDict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # for i in range(min(20, len(cosSimilarityDict))):\n",
        "    #   maxSimDocID = max(cosSimilarityDict, key=cosSimilarityDict.get)\n",
        "    #   maxSimScore = cosSimilarityDict[maxSimDocID]\n",
        "    #   sorted_documents.append((maxSimDocID, maxSimScore))\n",
        "    return sorted_documents[:20]\n",
        "\n",
        "def compute_tf_idf_for_query(query, documents):\n",
        "    # Preprocess the query\n",
        "    preQuery = preprocess_query(query)\n",
        "\n",
        "    # Create a document object for the query\n",
        "    queryDoc = Document(\"query\", query)\n",
        "    # Compute TF-IDF for the query document\n",
        "    query_tf_idf = {}\n",
        "    termFreq = Counter(preQuery)\n",
        "    totalTerms = sum(termFreq.values())\n",
        "    for term, freq in termFreq.items():\n",
        "      tf = freq / totalTerms\n",
        "      ## for every term, calculate doc frequency\n",
        "      df = 0\n",
        "      for doc in documents:\n",
        "        if term in doc.text:\n",
        "          df += 1\n",
        "      if df > 0:\n",
        "        idf = math.log(len(documents) / (df))\n",
        "      else:\n",
        "        idf = 0\n",
        "      query_tf_idf[term] = tf * idf\n",
        "    return query_tf_idf\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Fetch documents from URL\n",
        "    url = 'https://raw.githubusercontent.com/sumonacalpoly/Datasets/main/documents.txt'\n",
        "    documents_collection = fetch_documents(url)\n",
        "\n",
        "    if documents_collection:\n",
        "        # Preprocess documents\n",
        "        documents_collection.preprocess_documents()\n",
        "        # Compute TF-IDF scores\n",
        "        tf_idf = documents_collection.compute_tf_idf()\n",
        "        # Save TF-IDF vectors to pickle file\n",
        "        documents_collection.save_to_pickle(\"tfidfVectors.pkl\")\n",
        "\n",
        "        # Read queries from file\n",
        "        queries_url = 'https://raw.githubusercontent.com/sumonacalpoly/Datasets/main/queries.txt'\n",
        "        queries_response = requests.get(queries_url)\n",
        "        if queries_response.status_code == 200:\n",
        "            # queries = [line.strip() for line in queries_response.text.splitlines()]\n",
        "            # Perform document retrieval for each query\n",
        "            queries_data = queries_response.text.strip().split('.I')[1:6]\n",
        "            queries = {}\n",
        "            for queryData in queries_data:\n",
        "              query_lines = queryData.split('.W')  # Split query lines\n",
        "              query_number = query_lines[0].strip()\n",
        "              query_text = query_lines[1].strip()\n",
        "              queries[query_number] = query_text\n",
        "              # queryTFIDF = compute_tf_idf_for_query(query, documents_collection.documents.values())\n",
        "              # similarDocs = compute_cosine_similarity(queryTFIDF, tf_idf)\n",
        "              # print(f\"Documents for query '{query}': {[doc[0] for doc in similarDocs[:20]]}\")\n",
        "            ## Doc retrieval\n",
        "            for query_number, query_text in queries.items():\n",
        "                # Compute TF-IDF representation for the query\n",
        "                queryTFIDF = compute_tf_idf_for_query(query_text, documents_collection.documents.values())\n",
        "                # Compute cosine similarity between the query and documents\n",
        "                similarDocs = compute_cosine_similarity(queryTFIDF, tf_idf)\n",
        "                # Print the top 20 relevant documents for the query\n",
        "                print(f\"Documents for query '{query_number}': {[doc[0] for doc in similarDocs[:20]]}\")\n",
        "        else:\n",
        "            print(\"Failed to retrieve queries file. Status code:\", queries_response.status_code)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "wBvDVJ4JD9KV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71f0465-073f-49b7-bfdc-495a0bae6d23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents for query '001': [13, 51, 184, 429, 792, 359, 1111, 686, 878, 486, 114, 327, 435, 875, 1144, 540, 700, 102, 100, 154]\n",
            "Documents for query '002': [51, 12, 792, 700, 429, 875, 884, 1147, 1111, 833, 883, 606, 1163, 726, 746, 804, 100, 114, 1158, 1379]\n",
            "Documents for query '004': [485, 181, 5, 144, 399, 542, 707, 91, 582, 425, 666, 350, 623, 944, 554, 347, 303, 395, 586, 90]\n",
            "Documents for query '008': [166, 1312, 236, 185, 1275, 575, 317, 1085, 375, 914, 435, 410, 1189, 656, 1077, 4, 570, 456, 1010, 1061]\n",
            "Documents for query '009': [103, 540, 26, 1272, 650, 1102, 813, 746, 1379, 568, 327, 1158, 360, 172, 401, 1066, 137, 1391, 573, 575]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8_UcO2k2ck0j"
      }
    }
  ]
}